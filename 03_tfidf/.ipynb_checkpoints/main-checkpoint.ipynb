{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "- bag of words: Vectors of word counts or frequencies.\n",
    "- bag of n-grams: Counts of word pairs (bigrams), triplets (trigrams), and so on.\n",
    "- tf-idf vectors: Word scores that better represent their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import copy\n",
    "import math\n",
    "\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances, cosine_similarity\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nlpia.data.loaders import kite_text, kite_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " 'got',\n",
       " 'to',\n",
       " 'the',\n",
       " 'store',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " 'Harry',\n",
       " ',',\n",
       " 'the',\n",
       " 'faster',\n",
       " ',',\n",
       " 'would',\n",
       " 'go',\n",
       " 'home',\n",
       " '.']"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "sentence = 'The faster Harry got to the store, the faster Harry, the faster, would go home.'\n",
    "tokens = tokenizer.tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('faster', 3), ('the', 3), (',', 3)]"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words = Counter(tokens)\n",
    "bag_of_words.most_common(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "times_harry_appear = bag_of_words['Harry']\n",
    "times_harry_appear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_words = len(bag_of_words)\n",
    "num_unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1667"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = times_harry_appear / num_unique_words\n",
    "round(tf, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = ['The faster Harry got to the store, the faster and faster Harry would get home.',\n",
    "        'Harry is hairy anf gaster than Jill.',\n",
    "        'Jill is not as hairy as Harry.']\n",
    "\n",
    "doc_tokens = []\n",
    "for doc in docs:\n",
    "    doc_tokens += [sorted(tokenizer.tokenize(doc.lower()))]\n",
    "len(doc_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([',', '.', 'and', 'faster', 'faster'], 33)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_doc_tokens = sum(doc_tokens, [])\n",
    "all_doc_tokens[:5], len(all_doc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([',', '.', 'and', 'anf', 'as'], 20)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lexicon = sorted(set(all_doc_tokens))\n",
    "lexicon[:5], len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_vector = OrderedDict((token, 0) for token in lexicon)\n",
    "document_tfidf_vectors = []\n",
    "\n",
    "for doc in docs:\n",
    "    vec = copy.copy(zero_vector)\n",
    "    tokens = tokenizer.tokenize(doc.lower())\n",
    "    token_counts = Counter(tokens)\n",
    "    \n",
    "    for key, value in token_counts.items():\n",
    "        docs_containing_key = 0\n",
    "        for _doc in docs:\n",
    "            if key in _doc:\n",
    "                docs_containing_key += 1\n",
    "        tf = value / len(lexicon)\n",
    "        if docs_containing_key:\n",
    "            idf = len(docs) / docs_containing_key\n",
    "        else:\n",
    "            idf = 0\n",
    "        \n",
    "        vec[key] = tf * idf # Why no math.log?\n",
    "    document_tfidf_vectors.append(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(v_dict, w_dict):\n",
    "    v = [val for val in v_dict.values()]\n",
    "    w = [val for val in w_dict.values()]\n",
    "    \n",
    "    dot_prod = 0\n",
    "    for v_i, w_i in zip(v, w):\n",
    "        dot_prod += v_i * w_i\n",
    "    \n",
    "    # Calculate the magnitude.\n",
    "    mag_1 = math.sqrt(sum([x * x for x in v]))\n",
    "    mag_2 = math.sqrt(sum([x * x for x in w]))\n",
    "    \n",
    "    return dot_prod / (mag_1 * mag_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'How long does it take to get to the store?'\n",
    "query_vec = copy.copy(zero_vector)\n",
    "tokens = tokenizer.tokenize(query.lower())\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "for key, value in token_counts.items():\n",
    "    docs_containing_key = 0\n",
    "    for _doc in docs:\n",
    "        if key in _doc.lower():\n",
    "            docs_containing_key += 1\n",
    "    if docs_containing_key == 0:\n",
    "        continue\n",
    "    tf = value / len(tokens)\n",
    "    idf = len(docs) / docs_containing_key\n",
    "    \n",
    "    query_vec[key] = tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5163977794943223\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "for vectors in document_tfidf_vectors:\n",
    "    print(cosine_sim(query_vec, vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.515\n",
      "1 0.000\n",
      "2 0.000\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=1)\n",
    "vectors = vectorizer.fit_transform(docs)\n",
    "\n",
    "res = vectorizer.transform([query])\n",
    "for i, vec in enumerate(vectors):\n",
    "    # print('euclidean: {:.3f}'.format(euclidean_distances(res, vec)[0][0]))\n",
    "    print(i, '{:.3f}'.format(cosine_similarity(res, vec)[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kite Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "stopwords = frozenset(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "tokens = tokenizer.tokenize(kite_text.lower())\n",
    "tokens = [word for word in tokens if word not in stopwords]\n",
    "token_counts = Counter(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kite', 16), (',', 15), ('kites', 8), ('wing', 5), ('lift', 4)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counts.most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07207207207207207, 0.06756756756756757, 0.036036036036036036]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_vectors = []\n",
    "\n",
    "doc_length = len(tokens)\n",
    "for key, value in token_counts.most_common():\n",
    "    document_vectors.append(value / doc_length)\n",
    "    \n",
    "document_vectors[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "kite_intro = kite_text.lower()\n",
    "intro_tokens = tokenizer.tokenize(kite_intro)\n",
    "intro_total = len(intro_tokens)\n",
    "\n",
    "kite_history = kite_history.lower()\n",
    "history_tokens = tokenizer.tokenize(kite_history)\n",
    "history_total = len(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_tf, history_tf = {}, {}\n",
    "\n",
    "intro_counts = Counter(intro_tokens)\n",
    "history_counts = Counter(history_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in intro is: 0.0441'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tf['kite'] = intro_counts['kite'] / intro_total\n",
    "f'Term Frequency of \"kite\" in intro is: {intro_tf[\"kite\"]:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"kite\" in history is: 0.0202'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_tf['kite'] = history_counts['kite'] / history_total\n",
    "f'Term Frequency of \"kite\" in history is: {history_tf[\"kite\"]:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"and\" in intro is: 0.0275'"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_tf['and'] = intro_counts['and'] / intro_total\n",
    "f'Term Frequency of \"and\" in intro is: {intro_tf[\"and\"]:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Term Frequency of \"and\" in history is: 0.0303'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_tf['and'] = history_counts['and'] / history_total\n",
    "f'Term Frequency of \"and\" in history is: {history_tf[\"and\"]:.4f}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
